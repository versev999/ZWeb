"""
å›¾åƒç”ŸæˆæœåŠ¡å±‚
æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œè¿œç¨‹APIä¸¤ç§è°ƒç”¨æ–¹å¼
"""

import base64
import httpx
import asyncio
import io
from pathlib import Path
from typing import Optional
from datetime import datetime
from abc import ABC, abstractmethod

from config import get_config


class ImageGenerationResult:
    """å›¾åƒç”Ÿæˆç»“æœ"""
    
    def __init__(
        self,
        success: bool,
        image_data: Optional[bytes] = None,
        image_base64: Optional[str] = None,
        file_path: Optional[str] = None,
        error: Optional[str] = None,
        generation_time: float = 0.0
    ):
        self.success = success
        self.image_data = image_data
        self.image_base64 = image_base64
        self.file_path = file_path
        self.error = error
        self.generation_time = generation_time
    
    def to_dict(self) -> dict:
        return {
            "success": self.success,
            "image_base64": self.image_base64,
            "file_path": self.file_path,
            "error": self.error,
            "generation_time": self.generation_time
        }


class BaseImageGenerator(ABC):
    """å›¾åƒç”Ÿæˆå™¨åŸºç±»"""
    
    @abstractmethod
    async def generate(
        self,
        prompt: str,
        negative_prompt: str = "",
        width: int = 512,
        height: int = 512,
        steps: int = 20,
        guidance_scale: float = 7.5,
        seed: Optional[int] = None
    ) -> ImageGenerationResult:
        """ç”Ÿæˆå›¾åƒ"""
        pass


class LocalModelGenerator(BaseImageGenerator):
    """æœ¬åœ°æ¨¡å‹å›¾åƒç”Ÿæˆå™¨"""
    
    def __init__(self, model_path: str, device: str = "cuda"):
        self.model_path = model_path
        self.device = device
        self._model = None
        self._pipe = None
    
    def _load_model(self):
        """åŠ è½½æœ¬åœ°æ¨¡å‹"""
        if self._pipe is not None:
            return
        
        try:
            import torch
            from diffusers import DiffusionPipeline
            
            print(f"\n{'='*60}")
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
            print(f"ç›®æ ‡è®¾å¤‡: {self.device}")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            model_path = Path(self.model_path)
            if not model_path.exists():
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
            
            # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
            if self.device == "cuda" and not torch.cuda.is_available():
                print("âš ï¸  è­¦å‘Š: CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPUæ¨¡å¼")
                self.device = "cpu"
            elif self.device == "cuda":
                print(f"âœ… æ£€æµ‹åˆ° GPU: {torch.cuda.get_device_name(0)}")
                print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            
            # åŠ è½½æ¨¡å‹
            print("ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹æ–‡ä»¶...")
            self._pipe = DiffusionPipeline.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                use_safetensors=True,
            )
            
            # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            print(f"ğŸš€ æ­£åœ¨å°†æ¨¡å‹ç§»åŠ¨åˆ° {self.device}...")
            self._pipe = self._pipe.to(self.device)
            
            # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
            if self.device == "cuda":
                # å¯ç”¨å†…å­˜ä¼˜åŒ–
                self._pipe.enable_attention_slicing()
                print("âœ… å·²å¯ç”¨ Attention Slicing å†…å­˜ä¼˜åŒ–")
                
                # å¦‚æœæ”¯æŒï¼Œå¯ç”¨ xformers
                try:
                    self._pipe.enable_xformers_memory_efficient_attention()
                    print("âœ… å·²å¯ç”¨ xformers å†…å­˜ä¼˜åŒ–")
                except Exception:
                    print("â„¹ï¸  xformers ä¸å¯ç”¨ï¼Œè·³è¿‡ï¼ˆä¸å½±å“æ­£å¸¸ä½¿ç”¨ï¼‰")
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
            print(f"{'='*60}\n")
            
        except ImportError as e:
            raise ImportError(
                f"âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åº“ã€‚\n"
                f"è¯·å®‰è£…: uv pip install torch diffusers transformers accelerate\n"
                f"é”™è¯¯è¯¦æƒ…: {e}"
            )
        except FileNotFoundError as e:
            raise FileNotFoundError(f"âŒ {e}")
        except Exception as e:
            raise RuntimeError(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    async def generate(
        self,
        prompt: str,
        negative_prompt: str = "",
        width: int = 512,
        height: int = 512,
        steps: int = 20,
        guidance_scale: float = 7.5,
        seed: Optional[int] = None
    ) -> ImageGenerationResult:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆå›¾åƒ"""
        import time
        import io
        import torch
        
        start_time = time.time()
        
        try:
            # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
            self._load_model()
            
            # è®¾ç½®éšæœºç§å­
            generator = None
            if seed is not None:
                generator = torch.Generator(device=self.device).manual_seed(seed)
            
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œæ¨¡å‹æ¨ç†ï¼ˆé¿å…é˜»å¡å¼‚æ­¥äº‹ä»¶å¾ªç¯ï¼‰
            loop = asyncio.get_event_loop()
            image = await loop.run_in_executor(
                None,
                self._generate_sync,
                prompt,
                negative_prompt,
                width,
                height,
                steps,
                guidance_scale,
                generator
            )
            
            # å°†PILå›¾åƒè½¬æ¢ä¸ºå­—èŠ‚
            img_byte_arr = io.BytesIO()
            image.save(img_byte_arr, format='PNG')
            image_bytes = img_byte_arr.getvalue()
            
            # è½¬æ¢ä¸ºbase64
            image_base64 = base64.b64encode(image_bytes).decode("utf-8")
            
            generation_time = time.time() - start_time
            
            return ImageGenerationResult(
                success=True,
                image_data=image_bytes,
                image_base64=image_base64,
                generation_time=generation_time
            )
        except Exception as e:
            return ImageGenerationResult(
                success=False,
                error=f"å›¾åƒç”Ÿæˆå¤±è´¥: {str(e)}",
                generation_time=time.time() - start_time
            )
    
    def _generate_sync(
        self,
        prompt: str,
        negative_prompt: str,
        width: int,
        height: int,
        steps: int,
        guidance_scale: float,
        generator
    ):
        """åŒæ­¥ç”Ÿæˆå›¾åƒï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        result = self._pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            num_inference_steps=steps,
            guidance_scale=guidance_scale,
            generator=generator,
        )
        
        # è¿”å›ç¬¬ä¸€å¼ å›¾åƒ
        return result.images[0]


class RemoteAPIGenerator(BaseImageGenerator):
    """è¿œç¨‹APIå›¾åƒç”Ÿæˆå™¨"""
    
    def __init__(self, endpoint: str, api_key: str):
        self.endpoint = endpoint
        self.api_key = api_key
    
    async def generate(
        self,
        prompt: str,
        negative_prompt: str = "",
        width: int = 512,
        height: int = 512,
        steps: int = 20,
        guidance_scale: float = 7.5,
        seed: Optional[int] = None
    ) -> ImageGenerationResult:
        """é€šè¿‡è¿œç¨‹APIç”Ÿæˆå›¾åƒ"""
        import time
        start_time = time.time()
        
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                payload = {
                    "prompt": prompt,
                    "negative_prompt": negative_prompt,
                    "width": width,
                    "height": height,
                    "steps": steps,
                    "guidance_scale": guidance_scale,
                }
                
                if seed is not None:
                    payload["seed"] = seed
                
                response = await client.post(
                    self.endpoint,
                    headers=headers,
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    # å‡è®¾APIè¿”å› base64 ç¼–ç çš„å›¾åƒ
                    image_base64 = result.get("image", result.get("data", ""))
                    
                    return ImageGenerationResult(
                        success=True,
                        image_base64=image_base64,
                        generation_time=time.time() - start_time
                    )
                else:
                    return ImageGenerationResult(
                        success=False,
                        error=f"APIé”™è¯¯: {response.status_code} - {response.text}",
                        generation_time=time.time() - start_time
                    )
                    
        except httpx.TimeoutException:
            return ImageGenerationResult(
                success=False,
                error="è¯·æ±‚è¶…æ—¶",
                generation_time=time.time() - start_time
            )
        except Exception as e:
            return ImageGenerationResult(
                success=False,
                error=str(e),
                generation_time=time.time() - start_time
            )


class ImageService:
    """å›¾åƒç”ŸæˆæœåŠ¡"""
    
    def __init__(self):
        self.config = get_config()
        self._generators: dict[str, BaseImageGenerator] = {}
    
    def _get_generator(self, model_id: str) -> BaseImageGenerator:
        """è·å–æˆ–åˆ›å»ºå›¾åƒç”Ÿæˆå™¨"""
        if model_id not in self._generators:
            model_config = self.config.get_model_config(model_id)
            if not model_config:
                raise ValueError(f"æœªæ‰¾åˆ°æ¨¡å‹é…ç½®: {model_id}")
            
            model_type = model_config.get("type", "")
            
            if model_type == "local":
                self._generators[model_id] = LocalModelGenerator(
                    model_path=model_config.get("model_path", ""),
                    device=model_config.get("device", "cuda")
                )
            elif model_type == "remote":
                self._generators[model_id] = RemoteAPIGenerator(
                    endpoint=model_config.get("endpoint", ""),
                    api_key=model_config.get("api_key", "")
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        return self._generators[model_id]
    
    async def generate_image(
        self,
        prompt: str,
        model_id: Optional[str] = None,
        negative_prompt: str = "",
        width: Optional[int] = None,
        height: Optional[int] = None,
        steps: Optional[int] = None,
        guidance_scale: Optional[float] = None,
        seed: Optional[int] = None,
        save_to_file: bool = False
    ) -> ImageGenerationResult:
        """ç”Ÿæˆå›¾åƒ"""
        # ä½¿ç”¨é»˜è®¤æ¨¡å‹
        if not model_id:
            model_id = self.config.default_model
        
        # è·å–æ¨¡å‹é…ç½®
        model_config = self.config.get_model_config(model_id)
        if not model_config:
            return ImageGenerationResult(
                success=False,
                error=f"æœªæ‰¾åˆ°æ¨¡å‹: {model_id}"
            )
        
        # ä½¿ç”¨æ¨¡å‹é»˜è®¤å‚æ•°å¡«å……ç¼ºå¤±å‚æ•°
        default_params = model_config.get("default_params", {})
        width = width or default_params.get("width", 512)
        height = height or default_params.get("height", 512)
        steps = steps or default_params.get("steps", 20)
        guidance_scale = guidance_scale or default_params.get("guidance_scale", 7.5)
        
        # è·å–ç”Ÿæˆå™¨å¹¶ç”Ÿæˆå›¾åƒ
        generator = self._get_generator(model_id)
        result = await generator.generate(
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            steps=steps,
            guidance_scale=guidance_scale,
            seed=seed
        )
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if save_to_file and result.success and result.image_data:
            file_path = self._save_image(result.image_data)
            result.file_path = file_path
        
        return result
    
    def _save_image(self, image_data: bytes) -> str:
        """ä¿å­˜å›¾åƒåˆ°æ–‡ä»¶"""
        output_config = self.config.output
        save_dir = Path(output_config.save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        filename = f"generated_{timestamp}.{output_config.format}"
        file_path = save_dir / filename
        
        with open(file_path, "wb") as f:
            f.write(image_data)
        
        return str(file_path)


# å…¨å±€æœåŠ¡å®ä¾‹
_service: Optional[ImageService] = None


def get_image_service() -> ImageService:
    """è·å–å›¾åƒæœåŠ¡å®ä¾‹"""
    global _service
    if _service is None:
        _service = ImageService()
    return _service
